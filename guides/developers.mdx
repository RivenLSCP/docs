---
title: "Guide for Developers"
description: "Build and integrate AI-referencing applications on SUI with Walrus storage"
---

## Overview

This guide helps developers integrate with our SUI-based AI reference platform. The platform leverages decentralized storage via Walrus for datasets, model artifacts, and inference results, while using off-chain compute services for heavy tasks. You can store dataset references, model metadata, and inference outputs on the SUI blockchain, ensuring transparency, integrity, and verifiability without overloading on-chain logic.

Key Components:

- **SUI Smart Contracts**: On-chain objects that store references, configuration, and data indices.
- **Walrus Decentralized Storage**: For hosting large datasets, model weights, and result files.
- **Off-Chain Compute Services**: For actual model training, inference, and vector operations. The chain only stores pointers, keys, and verifiable proofs of results.

## Quick Start

### Installation
Install the SDK (off-chain client) to interact with SUI and Walrus references:
<CodeGroup>
```bash npm
npm install @sera/sdk
```

```bash yarn
yarn add @sera/sdk
```

```bash pip
pip install sera-ai
```
</CodeGroup>

### Basic Setup (Off-Chain Client)

```python
from sera import SeraClient

# Initialize client with your SUI network settings
client = SeraClient(
    sui_network="testnet",        # or "mainnet"
    walrus_gateway="https://walrus-gateway.example.com",
    sui_package_id="0x...packageId"
)

# Check connectivity (off-chain)
status = client.health_check()
print(f"Connection status: {status}")
```

## Core SDKs

<CardGroup cols={2}>
  <Card title="Data Management" icon="database">
    Maintain references to datasets stored via Walrus, register dataset metadata on SUI.
    ```python
    from sera.data import DatasetManager
    ```
  </Card>
  <Card title="Model Registry" icon="brain">
    Register and manage model metadata (weights, config) stored off-chain in Walrus.
    ```python
    from sera.model import ModelClient
    ```
  </Card>
  <Card title="Vector Indexing" icon="layer-group">
   Store and reference vector embeddings' metadata on-chain. Actual vector indexing runs off-chain.
    ```python
    from sera.vector import VectorIndex
    ```
  </Card>
  <Card title="Off-chain Training & Inference" icon="dumbbell">
    Trigger off-chain tasks. Upload final trained model references and inference results to Walrus, update SUI records.
    ```python
    from sera.jobs import ComputeTask
    ```
  </Card>
</CardGroup>

## Authentication

### Access Keys
To interact with Walrus and submit transactions to SUI, you'll need appropriate keys:
<Steps> 
  1. **Generate Key**
     Use `sera-cli` to generate your SUI keypair and fund it on testnet.
      ```bash
      sera-cli new-addr
      sera-cli faucet
      ```
  2. **Set Credentials in Code**
     ```python
     import sera
     
     sera.sui_private_key = "your_sera_private_key"
     sera.walrus_api_key = "your_walrus_api_key"
     ```

  3. **Verify Access**
     ```python
     client = sera.Client()
     permissions = client.get_permissions()
     print(f"Permissions: {permissions}")
     ```
</Steps>

## Working with Data

### Storing & Referencing Datasets
Datasets are stored off-chain using Walrus. On-chain, you'll store a dataset reference (CID) and associated metadata:
```python
from sera.data import DatasetManager

manager = DatasetManager()

# Upload dataset to Walrus
cid = manager.upload_dataset(
    data_path="./local_data",
    redundancy=3,
    encryption=True
)

# Register the dataset on SUI with metadata
tx_digest = manager.register_dataset_on_sui(
    cid=cid,
    name="NLP Training Set",
    category="nlp"
)

print(f"Dataset registered with transaction: {tx_digest}")

# List datasets from on-chain references
datasets = manager.list_datasets(category="nlp", limit=10)
```

### Vector Operations
Vector operations (like similarity search) happen off-chain. Store embeddings and their metadata on Walrus, and keep a vector index reference on-chain:
```python
from sera.vector import VectorIndex

index = VectorIndex(
    index_name="my_embeddings_index",
    dimension=768
)

# Upload embeddings to Walrus and register on-chain
embeddings_cid = index.upload_vectors(
    vectors=embeddings, 
    metadata=metadata,
    batch_size=1000
)

index_ref_tx = index.register_index_on_sui(
    cid=embeddings_cid,
    description="Tech embeddings"
)

# Query off-chain and verify results on-chain
results = index.query_off_chain(
    query_vector=query_vector,
    top_k=10,
    filter={"category": "technology"}
)
```

## Model Management
### Registering Models
Instead of deploying GPU/CPU instances on-chain, model training and serving are done by trusted off-chain providers. Store model weights and configuration in Walrus, record references on SUI:
```python
from sera.model import ModelRegistry

registry = ModelRegistry()

# Upload model weights to Walrus
model_cid = registry.upload_model_weights(
    model_path="./model_weights",
    encryption=True
)

# Register model on SUI
model_tx = registry.register_model(
    cid=model_cid,
    model_name="MyNLPModel",
    version="1.0.0",
    metadata={"description": "A transformer-based NLP model"}
)
```

### Inference and Training Jobs
For inference or training, create a job on an off-chain compute service. Once completed, results are uploaded to Walrus and the SUI state is updated:
```python
from sera.model import ComputeTask

task = ComputeTask(
    model_name="MyNLPModel",
    inputs=["example input 1", "example input 2"]
)

# Trigger off-chain inference job
job_id = task.submit_job()

# Poll for completion (off-chain)
status = task.check_status(job_id)
if status == "completed":
    # Download results from Walrus
    predictions = task.retrieve_results(job_id)
    print(predictions)
    
    # Optionally store summarized results back on SUI
    task.record_inference_result_on_sui(job_id, summary="Inference on 2 inputs")
```

### Storage Management with Walrus

```python
from sera.storage import WalrusClient

storage = WalrusClient(api_key="your_walrus_api_key")

# Store arbitrary data (e.g. processed dataset)
cid = storage.store(
    data_path="./processed_data",
    redundancy=3,
    encryption=True
)

# Retrieve data off-chain
storage.retrieve(
    cid=cid,
    destination="./retrieved_data"
)
```
Record the CID and any related metadata on SUI to ensure transparency and immutability:
```python
from sera.data import DatasetManager
manager.update_dataset_reference_on_sui(dataset_id="ds_123", cid=cid)
```
## Monitoring & Analytics
On-chain, you can track references to usage records or proofs of computation. Off-chain services can be used to measure performance, cost, and other analytics, then commit signed summaries to SUI.

```python
from sera.analytics import Analytics

analytics = Analytics()

# Get usage metrics from off-chain and verify references on-chain
metrics = analytics.get_off_chain_metrics(
    start_date="2024-01-01",
    end_date="2024-01-31",
    metrics=["requests", "compute_hours", "storage_usage"]
)

# Commit a usage summary hash to SUI for audit
commit_tx = analytics.commit_summary_to_sui(metrics)
```

## Error Handling

```python
from sera.exceptions import SeraException

try:
    result = client.risky_operation()
except SeraException as e:
    if e.code == "rate_limit_exceeded":
        # Handle rate limiting
        pass
    elif e.code == "insufficient_permissions":
        # Handle permission issues
        pass
    else:
        # Handle other errors
        raise
```

## Best Practices

<AccordionGroup>
  <Accordion title="Rate Limiting">
    - Implement backoff when calling off-chain services
    - Cache repetitive responses off-chain
    - Use batch operations where possible
    - Keep a record of call frequency on SUI for transparency
  </Accordion>
  
  <Accordion title="Security">
    - Rotate off-chain API keys regularly
    - Use environment variables for sensitive data
    - Leverage SUI capabilities for immutable references
    - Encrypt data at rest on Walrus
  </Accordion>
</AccordionGroup>

## Support & Resources

<CardGroup cols={2}>
  <Card title="API Reference" icon="book-open">
    [View detailed API documentation](/api-reference)
  </Card>
  <Card title="Sample Code" icon="code">
    [Browse example projects](https://github.com/sera-xyz/examples)
  </Card>
  <Card title="Community" icon="discord">
    [Join developer community](https://discord.gg/datasphere)
  </Card>
  <Card title="Support" icon="headset">
    [Get technical help](mailto:dev-support@sera.xyz)
  </Card>
</CardGroup>

<Note>
For enterprise support and custom integrations, contact our developer relations team at dev-relations@sera.xyz
</Note> 